{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE 372 Assignment 3\n",
    "\n",
    "#### Name:  \n",
    "#### Date: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# The only libraries you should need for this assignment. \n",
    "# Execute this cell first. (You should get a sine wave.)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(0,1.0, 0.01)\n",
    "y = np.sin(2*np.pi*x)\n",
    "plt.plot(x,y)\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Example plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question I: Minhashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "We can describe a read as a set of unique overlapping $k$-mers, and we would expect similar reads to have similar sets. Write a function that takes $k$ and a read as inputs and outputs a dictionary indicating which of the $4^k$ $k$-mers are in the read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Write a function to compute the Jaccard similarity between two dictionaries outputted by your function from part 1. Using this function and the one you wrote for part 1, compute the Jaccard similarity between the reads <tt>CATGGACCGACCAG</tt> and <tt>GCAGTACCGATCGT</tt> for $k = 3$. What is the runtime complexity of your function? If you have $N$ reads of length $L$ each, what is the worst-case runtime complexity for computing the Jaccard similarity between every pair of reads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question II: Haplotype phasing coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\n",
    "\n",
    "For $\\epsilon = \\delta = 0.01$, plot $N^*$ as a function of $n$, the number of SNP's. How does $N^*$ scale with $n$ as $n$ grows? Linearly, sublinearly or superlinearly? Can you give an intuitive explnation for your answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\n",
    "\n",
    "For $\\epsilon = 0.1, n=100,000$, plot $N^*$ as a function of $\\delta$, the read error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question III: RNA-seq Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "Implement the EM algorithm for the error-free case where all transcripts have the same length. Fill in the code in the last cell of the ipython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was adapted from a lab designed for UC Berkeley's EE126 course\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "t = ['ATCTCGACGCACTGC', 'GAGTTCGAACTCTTC', 'AGAGTTCCAGTGTCA',\n",
    "     'AAAGCTCACTGCGGA', 'AGCGATATCAGAGTD']\n",
    "K = len(t) # Number of transcripts\n",
    "\n",
    "rho = np.random.rand(K)\n",
    "rho /= sum(rho)\n",
    "print rho\n",
    "# This rho is unknown to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample pmf according to rho\n",
    "def rand_choice( pmf ): \n",
    "    v = np.random.rand()\n",
    "    tmp = 0\n",
    "    for i in range(len(pmf)):\n",
    "        each_val = pmf[i]\n",
    "        tmp += each_val\n",
    "        if v <= tmp:\n",
    "            return i\n",
    "\n",
    "# randomly generate a read from a randomly chosen transcript\n",
    "def random_read( t, rho, L ): \n",
    "    chosen_seq = t[rand_choice( rho )]\n",
    "    start_idx = np.random.randint(0,len(chosen_seq) - L,1 )\n",
    "    end_idx = start_idx + L\n",
    "    return chosen_seq[ start_idx:end_idx ]\n",
    "        \n",
    "N = 1000 # Number of reads\n",
    "L = 5    # Length of each read\n",
    "\n",
    "reads = []\n",
    "for i in range(N):\n",
    "    reads.append( random_read(t, rho, L) )\n",
    "    \n",
    "print 'First 20 reads...\\n', reads[0:20]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_iter = 100 # number of E/M iterations\n",
    "\n",
    "# Naive alignment algorithm for no-error situation\n",
    "def find_all_alignments(t, read):\n",
    "    tmp = []\n",
    "    for j in range(len(t)):\n",
    "        if read in t[j]:\n",
    "            tmp.append(j)\n",
    "    return tmp\n",
    "\n",
    "# A: Compatibility List. A[i] is a list of transcripts that read i aligns to\n",
    "A = []\n",
    "for each_read in reads:\n",
    "    A.append( find_all_alignments(t, each_read) )\n",
    "\n",
    "Z = np.zeros([N, K]) # hidden state prior\n",
    "rho_est = (1/K)*np.ones([1,K])\n",
    "\n",
    "# ==============================================================================\n",
    "# Your EM code here. Save your final answer as rho_est\n",
    "\n",
    "# First initialize Z\n",
    "        \n",
    "# EM iterations\n",
    "for iter in range(N_iter):\n",
    "    # E step\n",
    "    rho_est =1\n",
    "    # M step\n",
    "    \n",
    "# ==============================================================================\n",
    "    \n",
    "print '(real) rho', rho\n",
    "print '(est.) rho', rho_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question IV: Single-cell RNA-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "X = np.loadtxt('Zeisel_expr.txt').T\n",
    "genes = np.loadtxt('Zeisel_genes.txt', delimiter='\\n', dtype=str)\n",
    "labels = np.loadtxt('Zeisel_labels.txt', delimiter='\\n', dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "Plot two histograms: log(total counts) across cells and log(total counts) across genes. Why do we need to take a log here? What does this say about the data? Remove the columns of the matrix corresponding to genes that are expressed less than 25 times across all cells. How many genes are removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Transform the data by adding 1 to all entries and taking the log (this is common practice). Two common dimensionality reduction strategies are principal component analysis (PCA) and t-distributed stochastic neighbor embedding (tSNE). You can implement PCA using the Python [sklearn package](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). We recommend [this](https://github.com/danielfrg/tsne) tSNE package for generating a tSNE embedding. List a pro and a con of each of the two methods. Create the following three plots and report what you see (for all plots, color points by their label):\n",
    "- Run PCA and plot the first two principal components\n",
    "- Run tSNE and plot the first two tSNE components (Note: this may take a few minutes)\n",
    "- Run tSNE on the top 50 principal components and plot the first two tSNE components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "Using sklearn, we will test 4 different [clustering algorithms]((http://scikit-learn.org/stable/modules/clustering.html) on the dataset and evaluate their performance using the true labels provided by the authors. The four clustering methods are K-means clustering, spectral clustering, affinity propagation, and density-based spatial clustering of applications with noise (DBSCAN). Using the top 50 principal components for the log-transformed data, for each clustering method:\n",
    "- Describe the hyperparameters one needs to tune\n",
    "- Tune the hyperparameters until you obtain reasonable labels (i.e. you obtain about 9 clusters)\n",
    "- Report your final hyperparameters\n",
    "- Compare your labels to the true labels using sklearn's [adjusted Rand Index](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
