\documentclass{article}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{float}
\usepackage{hyperref}
\usepackage[normalem]{ulem}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usetikzlibrary{external}
\tikzset{external/force remake}
\tikzexternalize

\begin{document}
\section*{Recap}
Last time, we introduced a model for the errors introduced by the sequencing process as:

\[y^A = Qs^A + n\]

where $s^A$ is the 0-1 vector describing the locations of A's in the DNA sequence, $Q$ is 
a matrix representing errors in the read process, and $n$ is $\mathcal{N}(0, \sigma^2)$ Gaussian noise. $y^A$ 
is the data we observe, in the form of intensities of light captured from photos of the
assay plate.

We considered models of the form:

\tikzset{
block/.style = {draw, fill=white, rectangle, minimum height=3em, minimum width=3em},
state/.style = {draw, fill=white, circle},
tmp/.style  = {coordinate}, 
sum/.style= {draw, fill=white, circle, node distance=1cm},
input/.style = {},
output/.style= {},
pinstyle/.style = {pin edge={to-,thin,black}
}
}



\begin{figure}[H]
\centering
\begin{tikzpicture}[auto, node distance=2cm,>=latex']
    \node [input] (yA) {$y^A$};
    \node [block, right of = yA] (H) {$H$};
    \node [block, right of=H] (threshold) {
    \begin{tikzpicture}
        \draw (-0.5, -0.5) -- (0, -0.5);
        \draw (0, -0.5) -- (0, 0.5);
        \draw (0, 0.5) -- (0.5, 0.5);
    \end{tikzpicture}
    };
    \node [output, right of=threshold] (sA) {$\hat s^A$};
    \draw [->] (yA) -- (H);
    \draw [->] (H) -- (threshold);
    \draw [->] (threshold) -- (sA);
\end{tikzpicture}
\end{figure}

But the problem with these models is that they ignore the fact that errors propagate
and therefore symbol-by-symbol decoding is suboptimal. 

\section*{Maximum Likelihood}
To find an optimal decoder, we appeal to the ideal of \textbf{maximum likelihood}, which asks us to find the sequence $s^A$
that is the solution of the problem:

\[ \max_{s^A \in \{0, 1\}^L} p(y^A | s^A) \]

Since $y^A | s^A \sim \mathcal{N}(Qs^A, \sigma^2)$, we have:
\[ p(y^A | s^A) = \frac{1}{(2\pi)^{n/2} \sigma} \exp \left( {-\frac{1}{2\sigma^2} \|y^A - Qs^A\|^2} \right) \]

Our optimization problem then reduces to:
\[ \min_{s^A \in \{0, 1\}^L}  \|y^A - Qs^A\|^2 \]

This is a discrete optimization problem with brute-force complexity $O(2^L)$, which is intractable
even for relatively small $L$ ($\sim 200)$.

However, we can exploit the following fact about the matrix $Q$. Remember that the values of the off-diagonals of $Q$ get
exponentially smaller as we move away from the diagonal - this is essentially because for a value of 1 at $s^A(i)$ to influence
the reading of $y^A(j)$, there would need to be $\left| i-j \right|$ independent failures in the sequencing process, each with its own
(small) probability.

Therefore, it is reasonable to approximate the matrix $Q$ as \textbf{band-diagonal}. We consider the case when only the main diagonal and the first lower
diagonal $Q$ (i.e. $Q_{i(i-1)}$, for $i = 2...,L$) are nonzero, with the rest assumed to be zero. For simplicity, in the following section, we
drop the superscript $A$ from $y^A$ and $s^A$. 

Given this band-diagonal assumption, the equation
\[ y = Qs + n \]
can be expanded as:
\begin{align*}
y(1) &= Q_{11} s(1) + n(1) \\
y(2) &= Q_{22} s(2) + Q_{21} s(1) + n(2) \\
y(3) &= Q_{33} s(3) + Q_{32} s(2) + n(2) \\
&\vdots
\end{align*}
Now, we can express the objective $\| y - Qs \|$ as:
\[ \sum_{i = 2}^L \left[ y(i) - Q_{ii} s(i) - Q_{i(i-1)}s(i-1) \right]^2 + (y(1) - Q_{11}s(1))^2 \]

The magical thing is that we can minimize this objective efficiently!

\section*{The Key Idea: State}
To solve this problem, we introduce the idea of a \textbf{state}. Noticing that in the objective above,
each term of the sum only depends on the value of $(s(i), s(i-1))$ - we will use this pair as our state.
Alternatively, we can think of the state 
\[(s(i), s(i-1))\]
 as encapsulating all the information we need
to produce a single (noisy) measurement $y(i)$ (because of the band-diagonal structure
of our matrix $Q$).

Now, instead of a sequence of 0's and 1's $s(1), s(2),...,s(L)$, we have a sequence of states
$(s(1), s(2)), (s(2), s(3)), ...$. In the first sequence, the value of $s(i)$ does not depend on the value of $s(i-1)$; however
in the second sequence, not all state transitions are allowed. They are summarized in the following diagram:

\begin{figure}[H]
\centering
\begin{tikzpicture}[auto, node distance=2cm,>=latex']
    \node [state] (00) {$0, 0$};
    \node [state, right of = 00] (10) {$1, 0$};
    \node [state, below of = 10] (11) {$1, 1$};
    \node [state, below of= 00] (01) {$0, 1$};
    \draw[->] (00) -- (10);
    \draw[->] (10) -- (11);
    \draw[->] (11) -- (01);
    \draw[->] (01) -- (00);
    \draw[->] (00) to [out = 180, in=90, looseness=4] (00);
    \draw[->] (11) to [out = 0, in=270, looseness=4] (11);
    \draw[->] (01) to [out = 60, in = 210] (10);
    \draw[->] (10) to [out = 240, in = 30] (01);
\end{tikzpicture}
\end{figure}

The idea of state is powerful
because it allows us to reformulate our problem as a shortest-path problem. 
Let the \textbf{cost of a state} $(s(i), s(j))$ be the value of $(y(i) - Q_{ii} s(i) - Q_{i(i-1)}s(i-1))^2$ (or $(y(1) - Q_{11}s(1))^2$ for $i = 1$).
Minimizing our objective corresponds to finding the minimum cost path (from left to right) through the \textbf{trellis}:
\begin{figure}[H]
\centering
\begin{tikzpicture}[auto, node distance=1.5cm], >=latex']
\node [state] (00a) {$00$};
\node [state, below of=00a] (01a) {$01$};
\node [state, below of=01a] (10a) {$10$};
\node [state, below of = 10a] (11a) {$11$};
\node [input, below of = 11a] (ia) {$i = 1$};

\node [state, right of=00a, node distance=2cm] (00b) {$00$};
\node [state, below of=00b] (01b) {$01$};
\node [state, below of=01b] (10b) {$10$};
\node [state, below of = 10b] (11b) {$11$};
\node [input, below of = 11b] (ib) {$i = 2$};

\node [state, right of=00b, node distance=2cm] (00c) {$00$};
\node [state, below of=00c] (01c) {$01$};
\node [state, below of=01c] (10c) {$10$};
\node [state, below of = 10c] (11c) {$11$};
\node [input, below of = 11c] (ic) {$i = 3$};

\node [right of=00c, node distance=2cm] (00d) {$\hdots$};
\node [below of=00d] (01d) {$\hdots$};
\node [below of=01d] (10d) {$\hdots$};
\node [below of = 10d] (11d) {$\hdots$};

\node [state, right of=00d, node distance=2cm] (00e) {$00$};
\node [state, below of=00e] (01e) {$01$};
\node [state, below of=01e] (10e) {$10$};
\node [state, below of = 10e] (11e) {$11$};
\node [input, below of = 11e] (ie) {$i = L$};


\draw[->] (00a) -- (00b);
\draw[->] (00a) -- (10b);
\draw[->] (10a) -- (01b);
\draw[->] (10a) -- (11b);
\draw[->] (01a) -- (00b);
\draw[->] (01a) -- (10b);
\draw[->] (11a) -- (01b);
\draw[->] (11a) -- (11b);

\draw[->] (00b) -- (00c);
\draw[->] (00b) -- (10c);
\draw[->] (10b) -- (01c);
\draw[->] (10b) -- (11c);
\draw[->] (01b) -- (00c);
\draw[->] (01b) -- (10c);
\draw[->] (11b) -- (01c);
\draw[->] (11b) -- (11c);

\draw[->] (00c) -- (00d);
\draw[->] (00c) -- (10d);
\draw[->] (10c) -- (01d);
\draw[->] (10c) -- (11d);
\draw[->] (01c) -- (00d);
\draw[->] (01c) -- (10d);
\draw[->] (11c) -- (01d);
\draw[->] (11c) -- (11d);

\draw[->] (00d) -- (00e);
\draw[->] (00d) -- (10e);
\draw[->] (10d) -- (01e);
\draw[->] (10d) -- (11e);
\draw[->] (01d) -- (00e);
\draw[->] (01d) -- (10e);
\draw[->] (11d) -- (01e);
\draw[->] (11d) -- (11e);

\end{tikzpicture}
\end{figure}

Each column represents a state $s(i)$, and each state within that column is associated with a cost $c_i(s(i), y(i))$ that
is a function of our estimate of the state $s(i)$ and our observation $y(i)$. The \textbf{minimum cost path} is the path
through the graph from left to right that minimizes the sum of the costs of the states through which it passes.
This minimum cost path corresponds exactly to the sequence $s$ that minimizes our objective function!

This problem can be solved using dynamic programming (the \textbf{Viterbi Algorithm}).
in $O(L2^b)$ time, where $b$ is the number of nonzero diagonals of our $Q$ matrix (and hence, $2^b$ is
the number of states at each time step of the trellis).
For more info, see \url{https://en.wikipedia.org/wiki/Convolutional_code}.

\section*{Long Read Technologies}
We now turn to the next generation of sequencing technologies, which are capable of producing much longer reads (hence the name,
\textbf{long read} technologies), albeit with higher (10-20\%) error rates. These methods are generally capable of quick sequencing
rates, and do not require PCR (i.e. they do \textbf{single molecule sequencing}).

\subsection*{Nanopore}
See video.

Each double-stranded DNA fragment is bound to a special enzyme. The enzyme binds to a nanopore, which is affixed
to a substrate. The enzyme then unzips and advances the DNA through the nanopore, one base at a time.
A voltage across the nanopore produces a small electrical current - the difference in resistances of
different bases allows the DNA sequence to be determined based on this varying current signal.

There are several issues with this technology that make it a challenging (read: interesting) signal processing problem.
\begin{itemize}
\item The enzyme's job is to advance the DNA forward at a relatively constant rate. However, sometimes the enzyme
moves the DNA forward faster or slower, or even moves it backwards!
\item The resistance across the nanopore is a function of the \textbf{DNA context} - $\sim4$ or so bases - rather than
an individual nucleotide.
\end{itemize}

For example, given a sequence $s = \text{AGCTTCA...}$, we're actually looking at a current signal that is the function of
the \textbf{4-mer sequence} 
\[ \text{AGCT, GCTT, CTTC, TTCA,...}\]
possibly with skipping and/or backtracking, so the actual current signal might be a function of
\[ \text{AGCT, GCTT, \sout{CTTC}, TTCA, ...} \]

We will model this channel as follows:

\begin{figure}[H]
\centering
\begin{tikzpicture}[auto, node distance=3cm,>=latex']
    \node [input, align = center] (DNA) {DNA\\ $s_i \in \{A, G, C, T\}$\\ $i = 1,2,3,...$};
    \node [block, right of = DNA, align=center] (b2c) {\sout{windowing}\\ Base-to-context};
    \node [input, right of =b2c, align = center] (4mer) {DNA $4$-mers\\ $z_i \in \{A, G, C, T\}^4$};
    \node [block, right of=4mer, align=center] (skip) {Random skips,\\ backtracking};
    \node [input, below of=DNA, node distance=1.5cm, align = center] (4merskip) {Modified \\ 4-mer sequence \\ $z_1, z_2, z_4, ...$};
    \node [block, right of=4merskip] (nanopore) {Nanopore};
    \node [output, right of=nanopore] (current) {Current signal};
    \coordinate[right of=skip] (cont1);
    \coordinate[left of = 4merskip] (cont2);
    \draw[->] (DNA) -- (b2c);
    \draw[->] (b2c) -- (4mer);
    \draw[->] (4mer) -- (skip);
    \draw[->] (skip) -- (cont1);
    \draw[->] (cont2) -- (4merskip);
    \draw[->] (4merskip) -- (nanopore);
    \draw[->] (nanopore) -- (current);
\end{tikzpicture}
\end{figure}

How can we reconstruct  the 4-mer sequence from a current signal? We return to the idea of state, discussed previously, except
that this time, our state $z_i$ takes values in the set of the 16 possible 4-mers $\{A, G, C, T\}^4$. At each timestep, we collect a noisy measurement of this
state $z_i$, and we can produce a cost that allows us (just like before) to use the Viterbi algorithm to find the maximum likelihood decoding of our sequence
of 4-mers.

\subsection*{Zero-mode Waveguide (PacBio)}
See video.





\end{document}